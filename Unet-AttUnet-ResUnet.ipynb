{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9601125,"sourceType":"datasetVersion","datasetId":5857357}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Defining DataLoader","metadata":{}},{"cell_type":"code","source":"from skimage import io\nimg = io.imread('/kaggle/input/marida-m/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_0.tif')\nmask = io.imread('/kaggle/input/marida-m/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_0_cl.tif')\nprint(img.shape)\nprint(mask.shape)\nprint(img.dtype)\nmask = mask.astype('int64')\nprint(mask.dtype)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:32:43.362421Z","iopub.execute_input":"2024-12-02T14:32:43.364272Z","iopub.status.idle":"2024-12-02T14:32:43.448808Z","shell.execute_reply.started":"2024-12-02T14:32:43.364147Z","shell.execute_reply":"2024-12-02T14:32:43.447450Z"}},"outputs":[{"name":"stdout","text":"(256, 256, 11)\n(256, 256)\nfloat32\nint64\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n\nimport os\nimport torch\nimport random\nimport numpy as np\nfrom tqdm import tqdm\n#from osgeo import gdal\nfrom skimage import io\nfrom os.path import dirname as up\nfrom torch.utils.data import Dataset\nimport torchvision.transforms.functional as F\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Pixel-Level class distribution (total sum equals 1.0)\nclass_distr = torch.Tensor([0.00452, 0.00203, 0.00254, 0.00168, 0.00766, 0.15206, 0.20232,\n 0.35941, 0.00109, 0.20218, 0.03226, 0.00693, 0.01322, 0.01158, 0.00052])\n\nbands_mean = np.array([0.05197577, 0.04783991, 0.04056812, 0.03163572, 0.02972606, 0.03457443,\n 0.03875053, 0.03436435, 0.0392113,  0.02358126, 0.01588816]).astype('float32')\n\nbands_std = np.array([0.04725893, 0.04743808, 0.04699043, 0.04967381, 0.04946782, 0.06458357,\n 0.07594915, 0.07120246, 0.08251058, 0.05111466, 0.03524419]).astype('float32')\n\n###############################################################\n# Pixel-level Semantic Segmentation Data Loader               #\n###############################################################\n#dataset_path = os.path.join(up(up(up(__file__))), 'data')\ndataset_path = '/kaggle/input/marida-m/'\n\nclass GenDEBRIS(Dataset): # Extend PyTorch's Dataset class\n    def __init__(self, mode = 'train', transform=None, standardization=None, path = dataset_path, agg_to_water= True):\n        \n        if mode=='train':\n            self.ROIs = np.genfromtxt(os.path.join(path, 'splits', 'train_X.txt'),dtype='str')\n                \n        elif mode=='test':\n            self.ROIs = np.genfromtxt(os.path.join(path, 'splits', 'test_X.txt'),dtype='str')\n                \n        elif mode=='val':\n            self.ROIs = np.genfromtxt(os.path.join(path, 'splits', 'val_X.txt'),dtype='str')\n            \n        else:\n            raise\n            \n        self.X = []           # Loaded Images\n        self.y = []           # Loaded Output masks\n            \n        for roi in tqdm(self.ROIs, desc = 'Load '+mode+' set to memory'):\n            \n            # Construct file and folder name from roi\n            roi_folder = '_'.join(['S2'] + roi.split('_')[:-1])               # Get Folder Name\n            roi_name = '_'.join(['S2'] + roi.split('_'))                      # Get File Name\n            roi_file = os.path.join(path, 'patches', roi_folder,roi_name + '.tif')       # Get File path\n            roi_file_cl = os.path.join(path, 'patches', roi_folder,roi_name + '_cl.tif') # Get Class Mask\n            \n            # Load Classsification Mask\n            #ds = gdal.Open(roi_file_cl)\n            #temp = np.copy(ds.ReadAsArray().astype(np.int64))\n            ds = io.imread(roi_file_cl)\n            temp = np.copy(ds.astype(np.int64))\n            # Aggregation\n            if agg_to_water:\n                temp[temp==15]=7          # Mixed Water to Marine Water Class\n                temp[temp==14]=7          # Wakes to Marine Water Class\n                temp[temp==13]=7          # Cloud Shadows to Marine Water Class\n                temp[temp==12]=7          # Waves to Marine Water Class\n            \n            # Categories from 1 to 0\n            temp = np.copy(temp - 1)\n            ds=None                   # Close file\n            \n            self.y.append(temp)\n            \n            # Load Patch\n            #ds = gdal.Open(roi_file)\n            #temp = np.copy(ds.ReadAsArray())\n            ds = io.imread(roi_file)\n            #ds = np.moveaxis(ds,[0,1,2],[2,0,1])\n            temp = np.copy(ds)\n            ds=None\n            self.X.append(temp)          \n\n        self.impute_nan = np.tile(bands_mean, (temp.shape[0],temp.shape[1],1))\n        self.mode = mode\n        self.transform = transform\n        self.standardization = standardization\n        self.length = len(self.y)\n        self.path = path\n        self.agg_to_water = agg_to_water\n        \n    def __len__(self):\n\n        return self.length\n    \n    def getnames(self):\n        return self.ROIs\n    \n    def __getitem__(self, index):\n        \n        img = self.X[index]\n        target = self.y[index]\n        \n        #img = np.moveaxis(img, [0, 1, 2], [2, 0, 1]).astype('float32')       # CxWxH to WxHxC\n        \n        #self.impute_nan = np.moveaxis(self.impute_nan, [0, 1, 2], [2, 0, 1]).astype('float32')       # CxWxH to WxHxC\n        \n        nan_mask = np.isnan(img)\n        img[nan_mask] = self.impute_nan[nan_mask]\n        \n        if self.transform is not None:\n            target = target[:,:,np.newaxis]\n            stack = np.concatenate([img, target], axis=-1).astype('float32') # In order to rotate-transform both mask and image\n        \n            stack = self.transform(stack)\n\n            img = stack[:-1,:,:]\n            target = stack[-1,:,:].long()                                    # Recast target values back to int64 or torch long dtype\n        \n        if self.standardization is not None:\n            img = self.standardization(img)\n            \n        return img, target\n    \n###############################################################\n# Transformations                                             #\n###############################################################\nclass RandomRotationTransform:\n    \"\"\"Rotate by one of the given angles.\"\"\"\n\n    def __init__(self, angles):\n        self.angles = angles\n\n    def __call__(self, x):\n        angle = random.choice(self.angles)\n        return F.rotate(x, angle)\n    \n###############################################################\n# Weighting Function for Semantic Segmentation                #\n###############################################################\ndef gen_weights(class_distribution, c = 1.02):\n    return 1/torch.log(c + class_distribution)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:32:43.451200Z","iopub.execute_input":"2024-12-02T14:32:43.451576Z","iopub.status.idle":"2024-12-02T14:32:43.502657Z","shell.execute_reply.started":"2024-12-02T14:32:43.451543Z","shell.execute_reply":"2024-12-02T14:32:43.501050Z"}},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":"# Defining U-net ++ (not working)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass NestedUNet(nn.Module):\n    def __init__(self, input_bands=11, output_classes=11, hidden_channels=16):\n        super(NestedUNet, self).__init__()\n        self.hidden_channels = hidden_channels\n\n        def double_conv(in_channels, out_channels):\n            return nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            )\n\n        # Contracting path\n        self.conv0_0 = double_conv(input_bands, hidden_channels)\n        self.conv1_0 = double_conv(hidden_channels, hidden_channels * 2)\n        self.conv2_0 = double_conv(hidden_channels * 2, hidden_channels * 4)\n        self.conv3_0 = double_conv(hidden_channels * 4, hidden_channels * 8)\n        self.conv4_0 = double_conv(hidden_channels * 8, hidden_channels * 8)\n\n        # Expanding path (nested convolutions)\n        self.conv0_1 = double_conv(hidden_channels + hidden_channels * 2, hidden_channels)\n        self.conv1_1 = double_conv(hidden_channels * 2 + hidden_channels * 4, hidden_channels * 2)\n        self.conv2_1 = double_conv(hidden_channels * 4 + hidden_channels * 8, hidden_channels * 4)\n        self.conv3_1 = double_conv(hidden_channels * 8 + hidden_channels * 8, hidden_channels * 8)\n\n        self.conv0_2 = double_conv(hidden_channels * 2 + hidden_channels, hidden_channels)\n        self.conv1_2 = double_conv(hidden_channels * 4 + hidden_channels * 2, hidden_channels * 2)\n        self.conv2_2 = double_conv(hidden_channels * 8 + hidden_channels * 4, hidden_channels * 4)\n\n        self.conv0_3 = double_conv(hidden_channels * 3 + hidden_channels, hidden_channels)\n        self.conv1_3 = double_conv(hidden_channels * 6 + hidden_channels * 2, hidden_channels * 2)\n\n        self.conv0_4 = double_conv(hidden_channels * 4 + hidden_channels, hidden_channels)\n\n        # Up-sampling layers\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n        # Final output layer\n        self.final = nn.Conv2d(hidden_channels, output_classes, kernel_size=1)\n\n    def forward(self, x):\n        # Contracting path\n        x0_0 = self.conv0_0(x)\n        x1_0 = self.conv1_0(self.up(x0_0))\n        x2_0 = self.conv2_0(self.up(x1_0))\n        x3_0 = self.conv3_0(self.up(x2_0))\n        x4_0 = self.conv4_0(self.up(x3_0))\n\n        # Nested path\n        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], dim=1))\n        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], dim=1))\n        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], dim=1))\n        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], dim=1))\n\n        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], dim=1))\n        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], dim=1))\n        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], dim=1))\n\n        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], dim=1))\n        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], dim=1))\n\n        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], dim=1))\n\n        # Final output layer\n        output = self.final(x0_4)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:32:43.504199Z","iopub.execute_input":"2024-12-02T14:32:43.504569Z","iopub.status.idle":"2024-12-02T14:32:43.522652Z","shell.execute_reply.started":"2024-12-02T14:32:43.504534Z","shell.execute_reply":"2024-12-02T14:32:43.521496Z"}},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":"# Defining Residual Unet","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\nimport torch\nfrom torch import nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels))\n\n        self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        residual = self.residual(x)\n        x = self.conv(x)\n        return nn.ReLU(inplace=True)(x + residual)\n\nclass Down(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Down, self).__init__()\n        self.maxpool = nn.MaxPool2d(2)\n        self.res_block = ResidualBlock(in_channels, out_channels)\n\n    def forward(self, x):\n        x = self.maxpool(x)\n        return self.res_block(x)\n\nclass Up(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Up, self).__init__()\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.res_block = ResidualBlock(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        x = torch.cat([x2, x1], dim=1)\n        return self.res_block(x)\n\nclass ResidualUNet(nn.Module):\n    def __init__(self, input_bands=11, output_classes=11, hidden_channels=16):\n        super(ResidualUNet, self).__init__()\n\n        # Initial Convolution Layer\n        self.inc = ResidualBlock(input_bands, hidden_channels)\n\n        # Contracting Path\n        self.down1 = Down(hidden_channels, 2 * hidden_channels)\n        self.down2 = Down(2 * hidden_channels, 4 * hidden_channels)\n        self.down3 = Down(4 * hidden_channels, 8 * hidden_channels)\n        self.down4 = Down(8 * hidden_channels, 8 * hidden_channels)\n\n        # Expanding Path\n        self.up1 = Up(16 * hidden_channels, 4 * hidden_channels)\n        self.up2 = Up(8 * hidden_channels, 2 * hidden_channels)\n        self.up3 = Up(4 * hidden_channels, hidden_channels)\n        self.up4 = Up(2 * hidden_channels, hidden_channels)\n\n        # Output Convolution Layer\n        self.outc = nn.Conv2d(hidden_channels, output_classes, kernel_size=1)\n\n    def forward(self, x):\n        # Contracting Path\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n\n        # Expanding Path\n        x6 = self.up1(x5, x4)\n        x7 = self.up2(x6, x3)\n        x8 = self.up3(x7, x2)\n        x9 = self.up4(x8, x1)\n\n        # Output Convolution Layer\n        logits = self.outc(x9)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:32:43.525406Z","iopub.execute_input":"2024-12-02T14:32:43.525864Z","iopub.status.idle":"2024-12-02T14:32:43.543581Z","shell.execute_reply.started":"2024-12-02T14:32:43.525826Z","shell.execute_reply":"2024-12-02T14:32:43.542351Z"}},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":"# Defining Attention U-Net","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super(AttentionGate, self).__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n    def forward(self, g, x):\n        # Align spatial dimensions if necessary\n        if g.size() != x.size():\n            g = self.upsample(g)\n\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        return x * psi\n\n\nclass Down(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Down, self).__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Up, self).__init__()\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass AttentionUNet(nn.Module):\n    def __init__(self, input_bands=1, output_classes=1, hidden_channels=64):\n        super(AttentionUNet, self).__init__()\n        # Initial Convolution Layer\n        self.inc = nn.Sequential(\n            nn.Conv2d(input_bands, hidden_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_channels),\n            nn.ReLU(inplace=True)\n        )\n\n        # Contracting Path\n        self.down1 = Down(hidden_channels, hidden_channels * 2)\n        self.down2 = Down(hidden_channels * 2, hidden_channels * 4)\n        self.down3 = Down(hidden_channels * 4, hidden_channels * 8)\n        self.down4 = Down(hidden_channels * 8, hidden_channels * 8)\n\n        # Expanding Path\n        self.up1 = Up(hidden_channels * 16, hidden_channels * 4)\n        self.up2 = Up(hidden_channels * 8, hidden_channels * 2)\n        self.up3 = Up(hidden_channels * 4, hidden_channels)\n        self.up4 = Up(hidden_channels * 2, hidden_channels)\n\n        # Attention Gates\n        self.att1 = AttentionGate(hidden_channels * 8, hidden_channels * 8, hidden_channels * 4)\n        self.att2 = AttentionGate(hidden_channels * 4, hidden_channels * 4, hidden_channels * 2)\n        self.att3 = AttentionGate(hidden_channels * 2, hidden_channels * 2, hidden_channels)\n\n        # Output Convolution Layer\n        self.outc = nn.Conv2d(hidden_channels, output_classes, kernel_size=1)\n\n    def forward(self, x):\n        # Contracting Path\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n\n        # Expanding Path\n        x6 = self.up1(x5, self.att1(x5, x4))\n        x7 = self.up2(x6, self.att2(x6, x3))\n        x8 = self.up3(x7, self.att3(x7, x2))\n        x9 = self.up4(x8, x1)\n\n        # Output Layer\n        logits = self.outc(x9)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:32:43.545595Z","iopub.execute_input":"2024-12-02T14:32:43.545953Z","iopub.status.idle":"2024-12-02T14:32:43.571492Z","shell.execute_reply.started":"2024-12-02T14:32:43.545922Z","shell.execute_reply":"2024-12-02T14:32:43.570386Z"}},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":"# Defining SegNet (not working)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass SegNet(nn.Module):\n    def __init__(self, input_channels, output_channels):\n        super(SegNet, self).__init__()\n        \n        # Encoder (Downsampling)\n        self.encoder = nn.Sequential(\n            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2),\n            \n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2),\n            \n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2),\n            \n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2),\n            \n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2)\n        )\n        \n        # Decoder (Upsampling)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True),\n            \n            nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True),\n            \n            nn.ConvTranspose2d(256, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True),\n            \n            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True),\n            \n            nn.ConvTranspose2d(64, output_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n        )\n    \n    def forward(self, x):\n        # Forward pass through encoder\n        x = self.encoder(x)\n        \n        # Forward pass through decoder\n        x = self.decoder(x)\n        \n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:32:43.573215Z","iopub.execute_input":"2024-12-02T14:32:43.573652Z","iopub.status.idle":"2024-12-02T14:32:43.594959Z","shell.execute_reply.started":"2024-12-02T14:32:43.573615Z","shell.execute_reply":"2024-12-02T14:32:43.593909Z"}},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":"# Defining U-Net","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\nimport torch\nimport numpy as np\nfrom torch import nn\nimport random\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\nclass Down(nn.Module):\n    # Contracting Layer\n    \n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    # Expanding Layer\n    \n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True))\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\nclass UNet(nn.Module):\n    \n    def __init__(self, input_bands = 11, output_classes = 11, hidden_channels=16):\n        super(UNet, self).__init__()\n        \n        # Initial Convolution Layer\n        self.inc = nn.Sequential(\n            nn.Conv2d(input_bands, hidden_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_channels),\n            nn.ReLU(inplace=True))\n        \n        # Contracting Path\n        self.down1 = Down(hidden_channels, 2*hidden_channels)\n        self.down2 = Down(2*hidden_channels, 4*hidden_channels)\n        self.down3 = Down(4*hidden_channels, 8*hidden_channels)\n        self.down4 = Down(8*hidden_channels, 8*hidden_channels)\n        \n        # Expanding Path\n        self.up1 = Up(16*hidden_channels, 4*hidden_channels)\n        self.up2 = Up(8*hidden_channels, 2*hidden_channels)\n        self.up3 = Up(4*hidden_channels, hidden_channels)\n        self.up4 = Up(2*hidden_channels, hidden_channels)\n        \n        # Output Convolution Layer\n        self.outc = nn.Conv2d(hidden_channels, output_classes, kernel_size=1)\n\n    def forward(self, x):\n        # Initial Convolution Layer\n        x1 = self.inc(x)\n        \n        # Contracting Path\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        \n        # Expanding Path\n        x6 = self.up1(x5, x4)\n        x7 = self.up2(x6, x3)\n        x8 = self.up3(x7, x2)\n        x9 = self.up4(x8, x1)\n        \n        # Output Convolution Layer\n        logits = self.outc(x9)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:32:43.597063Z","iopub.execute_input":"2024-12-02T14:32:43.597471Z","iopub.status.idle":"2024-12-02T14:32:43.622527Z","shell.execute_reply.started":"2024-12-02T14:32:43.597434Z","shell.execute_reply":"2024-12-02T14:32:43.621441Z"}},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":"# UTILS","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, jaccard_score, hamming_loss, label_ranking_loss, coverage_error\nimport sklearn.metrics as metr\nimport numpy as np\nimport pandas as pd\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# Evaluation for Pixel-level semantic segmentation\ndef Evaluation(y_predicted, y_true):\n\n    micro_prec = precision_score(y_true, y_predicted, average='micro')\n    macro_prec = precision_score(y_true, y_predicted, average='macro')\n    weight_prec = precision_score(y_true, y_predicted, average='weighted')\n    \n    micro_rec = recall_score(y_true, y_predicted, average='micro')\n    macro_rec = recall_score(y_true, y_predicted, average='macro')\n    weight_rec = recall_score(y_true, y_predicted, average='weighted')\n        \n    macro_f1 = f1_score(y_true, y_predicted, average=\"macro\")\n    micro_f1 = f1_score(y_true, y_predicted, average=\"micro\")\n    weight_f1 = f1_score(y_true, y_predicted, average=\"weighted\")\n        \n    subset_acc = accuracy_score(y_true, y_predicted)\n    \n    iou_acc = jaccard_score(y_true, y_predicted, average='macro')\n\n    info = {\n            \"macroPrec\" : macro_prec,\n            \"microPrec\" : micro_prec,\n            \"weightPrec\" : weight_prec,\n            \"macroRec\" : macro_rec,\n            \"microRec\" : micro_rec,\n            \"weightRec\" : weight_rec,\n            \"macroF1\" : macro_f1,\n            \"microF1\" : micro_f1,\n            \"weightF1\" : weight_f1,\n            \"subsetAcc\" : subset_acc,\n            \"IoU\": iou_acc\n            }\n    \n    return info\n\n# Evaluation for Multi-Label classification\ndef Evaluation_ML(y_predicted, predicted_probs, y_true):\n\n    micro_prec = precision_score(y_true, y_predicted, average='micro')\n    macro_prec = precision_score(y_true, y_predicted, average='macro')\n    sample_prec = precision_score(y_true, y_predicted, average='samples')\n    \n    micro_rec = recall_score(y_true, y_predicted, average='micro')\n    macro_rec = recall_score(y_true, y_predicted, average='macro')\n    sample_rec = recall_score(y_true, y_predicted, average='samples')\n        \n    macro_f1 = f1_score(y_true, y_predicted, average=\"macro\")\n    micro_f1 = f1_score(y_true, y_predicted, average=\"micro\")\n    sample_f1 = f1_score(y_true, y_predicted, average=\"samples\")\n        \n    subset_acc = accuracy_score(y_true, y_predicted)\n\n    hamming = hamming_loss(y_true, y_predicted)\n    coverage = coverage_error(y_true, y_predicted)\n    rank_loss = label_ranking_loss(y_true, y_predicted)\n\n    info = {\n            \"macroPrec\" : macro_prec,\n            \"microPrec\" : micro_prec,\n            \"samplePrec\" : sample_prec,\n            \"macroRec\" : macro_rec,\n            \"microRec\" : micro_rec,\n            \"sampleRec\" : sample_rec,\n            \"macroF1\" : macro_f1,\n            \"microF1\" : micro_f1,\n            \"sampleF1\" : sample_f1,\n            \"HammingLoss\" : hamming,\n            \"subsetAcc\" : subset_acc,\n            \"coverageError\" : coverage,\n            \"rankLoss\" : rank_loss\n            }\n    return info\n\ndef confusion_matrix(y_gt, y_pred, labels):\n\n    # compute metrics\n    cm      = metr.confusion_matrix  (y_gt, y_pred)\n    f1_macro= metr.f1_score          (y_gt, y_pred, average='macro')\n    mPA      = metr.recall_score      (y_gt, y_pred, average='macro')\n    OA      = metr.accuracy_score    (y_gt, y_pred)\n    UA      = metr.precision_score   (y_gt, y_pred, average=None)\n    PA      = metr.recall_score      (y_gt, y_pred, average=None)\n    f1      = metr.f1_score          (y_gt, y_pred, average=None)\n    IoC     = metr.jaccard_score     (y_gt, y_pred, average=None)\n    mIoC     = metr.jaccard_score    (y_gt, y_pred, average='macro')\n      \n    # confusion matrix\n    sz1, sz2 = cm.shape\n    cm_with_stats             = np.zeros((sz1+4,sz2+2))\n    cm_with_stats[0:-4, 0:-2] = cm\n    cm_with_stats[-3  , 0:-2] = np.round(IoC,2)\n    cm_with_stats[-2  , 0:-2] = np.round(UA,2)\n    cm_with_stats[-1  , 0:-2] = np.round(f1,2)\n    cm_with_stats[0:-4,   -1] = np.round(PA,2)\n    \n    cm_with_stats[-4  , 0:-2] = np.sum(cm, axis=0) \n    cm_with_stats[0:-4,   -2] = np.sum(cm, axis=1)\n    \n    # convert to list\n    cm_list = cm_with_stats.tolist()\n    \n    # first row\n    first_row = []\n    first_row.extend (labels)\n    first_row.append ('Sum')\n    first_row.append ('Recall')\n    \n    # first col\n    first_col = []\n    first_col.extend(labels)\n    first_col.append ('Sum')\n    first_col.append ('IoU')\n    first_col.append ('Precision')\n    first_col.append ('F1-score')\n    \n    # fill rest of the text \n    idx = 0\n    for sublist in cm_list:\n        if   idx == sz1:\n            sublist[-2]  = 'mPA:'\n            sublist[-1]  = round(mPA,2)\n            cm_list[idx] = sublist\n        elif   idx == sz1+1:\n            sublist[-2]  = 'mIoU:'\n            sublist[-1]  = round(mIoC,2)\n            cm_list[idx] = sublist\n            \n        elif idx == sz1+2:\n            sublist[-2]  = 'OA:'\n            sublist[-1]  = round(OA,2)\n            cm_list[idx] = sublist\n            \n        elif idx == sz1+3:\n            cm_list[idx] = sublist\n            sublist[-2]  = 'F1-macro:'\n            sublist[-1]  = round(f1_macro,2)    \n        idx +=1\n    \n    # Convert to data frame\n    df = pd.DataFrame(np.array(cm_list))\n    df.columns = first_row\n    df.index = first_col\n    \n    return df\n\ndef print_confusion_matrix_ML(confusion_matrix, class_label, ind_names, col_names):\n\n    df_cm = pd.DataFrame(confusion_matrix, index=ind_names, columns=col_names)\n    \n    df_cm.index.name = class_label\n    return df_cm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:32:43.624212Z","iopub.execute_input":"2024-12-02T14:32:43.624578Z","iopub.status.idle":"2024-12-02T14:32:43.653545Z","shell.execute_reply.started":"2024-12-02T14:32:43.624545Z","shell.execute_reply":"2024-12-02T14:32:43.652318Z"}},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Settings: Training settings. Here you have to set epochs, batch_size...","metadata":{}},{"cell_type":"code","source":"import os\nimport ast\nimport sys\nimport json\nimport random\nimport logging\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\nfrom os.path import dirname as up\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader\n\n#sys.path.append(up(os.path.abspath(__file__)))\n#from unet import UNet\n#from dataloader import GenDEBRIS, bands_mean, bands_std, RandomRotationTransform , class_distr, gen_weights\n\n#sys.path.append(os.path.join(up(up(up(os.path.abspath(__file__)))), 'utils'))\n#from metrics import Evaluation\n\nroot_path = '/kaggle/working/'#up(up(up(os.path.abspath(__file__))))\n\nlogging.basicConfig(filename=os.path.join(root_path, 'logs','log_unet.log'), filemode='a',level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s')\nlogging.info('*'*10)\n\ndef seed_all(seed):\n    # Pytorch Reproducibility\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \ndef seed_worker(worker_id):\n    # DataLoader Workers Reproducibility\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\n    \nimport ast \noptions = {}\noptions['agg_to_water'] = True #default=True, type=bool,  help='Aggregate Mixed Water, Wakes, Cloud Shadows, Waves with Marine Water'\noptions['mode']='train' # help='select between train or test ')\noptions['epochs']=45    # type=int, help='Number of epochs to run')\noptions['batch']=5      # default=5, type=int, help='Batch size')\noptions['resume_from_epoch']=0  # default=0, type=int, help='load model from previous epoch')\n\noptions['input_channels']=11 # default=11, type=int, help='Number of input bands')\noptions['output_channels']=11 # default=11, type=int, help='Number of output classes')\noptions['hidden_channels']=16 # default=16, type=int, help='Number of hidden features')\noptions['weight_param']= 1.03 # default=1.03, type=float, help='Weighting parameter for Loss Function')\n\n# Optimization\noptions['lr'] = 2e-4 # default=2e-4, type=float, help='learning rate')\noptions['decay']=0 # default=0, type=float, help='learning rate decay')\noptions['reduce_lr_on_plateau']=0 # default=0, type=int, help='reduce learning rate when no increase (0 or 1)')\noptions['lr_steps'] = '40' # default='[40]', type=str, help='Specify the steps that the lr will be reduced')\n\n# Evaluation/Checkpointing\noptions['checkpoint_path']='/kaggle/working/trained_models/' #default='/kaggle/working/trained_models/',to save checkpoints into (empty = this folder)')\noptions['eval_every']=1 # default=1, type=int, help='How frequently to run evaluation (epochs)')\n\n# misc\n#parser.add_argument('--num_workers', default=1, type=int, help='How many cpus for loading data (0 is the main process)')\n#parser.add_argument('--pin_memory', default=False, type=bool, help='Use pinned memory or not')\n#parser.add_nextargument('--prefetch_factor', default=1, type=int, help='Number of sample loaded in advance by each worker')\n#parser.add_argument('--persistent_workers', default=True, type=bool, help='This allows to maintain the workers Dataset instances alive.')\n#parser.add_argument('--tensorboard', default='tsboard_segm', type=str, help='Name for tensorboard run')\noptions['num_workers']=1\noptions['pin_memory']=False\noptions['prefetch_factor']=1\noptions['persistent_workers']=True\noptions['tensorboard']='tsboard_segm'\n#args = parser.parse_args()\n#options = vars(args)  # convert to ordinary dict\n\n# lr_steps list or single float\nlr_steps = ast.literal_eval(options['lr_steps'])\n#lr_steps = options['lr_steps']\nif type(lr_steps) is list:\n    pass\nelif type(lr_steps) is int:\n    lr_steps = [lr_steps]\nelse:\n    raise\n    \noptions['lr_steps'] = lr_steps\n\nlogging.info('parsed input parameters:')\nlogging.info(json.dumps(options, indent = 2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:32:43.657301Z","iopub.execute_input":"2024-12-02T14:32:43.658058Z","iopub.status.idle":"2024-12-02T14:32:43.677282Z","shell.execute_reply.started":"2024-12-02T14:32:43.657993Z","shell.execute_reply":"2024-12-02T14:32:43.675865Z"}},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":"## Training Section","metadata":{}},{"cell_type":"code","source":"import torchvision.transforms.functional as F\n\nseed_all(0)\ng = torch.Generator()\ng.manual_seed(0)\n\n# Tensorboard\nwriter = SummaryWriter(os.path.join(root_path, 'logs', options['tensorboard']))\n\n# Transformations\ntransform_train = transforms.Compose([transforms.ToTensor(),\n                                RandomRotationTransform([-90, 0, 90, 180]),\n                                transforms.RandomHorizontalFlip()])\n\ntransform_test = transforms.Compose([transforms.ToTensor()])\n\nstandardization = transforms.Normalize(bands_mean, bands_std)\n\n# Construct Data loader\ndataset_train = GenDEBRIS('train', transform=transform_train, standardization=standardization, agg_to_water=options['agg_to_water'])\ndataset_test = GenDEBRIS('val', transform=transform_test, standardization=standardization, agg_to_water=options['agg_to_water'])\n\ntrain_loader = DataLoader(dataset_train,\n                          batch_size=options['batch'],\n                          shuffle=True,\n                          num_workers=options['num_workers'],\n                          pin_memory=options['pin_memory'],\n                          prefetch_factor=options['prefetch_factor'],\n                          persistent_workers=options['persistent_workers'],\n                          worker_init_fn=seed_worker,\n                          generator=g)\n\ntest_loader = DataLoader(dataset_test,\n                         batch_size=options['batch'],\n                         shuffle=False,\n                         num_workers=options['num_workers'],\n                         pin_memory=options['pin_memory'],\n                         prefetch_factor=options['prefetch_factor'],\n                         persistent_workers=options['persistent_workers'],\n                         worker_init_fn=seed_worker,\n                         generator=g)\n\n# Use gpu or cpu\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\n\n# # SegNet\n# model = SegNet(input_channels=options['input_channels'], output_channels=options['output_channels'])\n\n\n# Residual U-Net\n# model =  ResidualUNet(input_bands=options['input_channels'],\n#              output_classes=options['output_channels'],\n#              hidden_channels=options['hidden_channels'])\n\n\n# Attention U-Net\n# model = AttentionUNet(input_bands=options['input_channels'],\n#              output_classes=options['output_channels'],\n#              hidden_channels=options['hidden_channels'])\n\n# U-Net\nmodel = UNet(input_bands=options['input_channels'],\n             output_classes=options['output_channels'],\n             hidden_channels=options['hidden_channels'])\n\n# U-Net++\n\n# model = NestedUNet(input_bands=options['input_channels'],\n#              output_classes=options['output_channels'],\n#              hidden_channels=options['hidden_channels'])\n\n\n# FCN\n# model = FCN(input_channels=options['input_channels'], output_classes=options['output_channels'])\n\nmodel.to(device)\n\n# Load model from specific epoch to continue the training or start the evaluation\nif options['resume_from_epoch'] > 1:\n    resume_model_dir = os.path.join(options['checkpoint_path'], str(options['resume_from_epoch']))\n    model_file = os.path.join(resume_model_dir, 'model.pth')\n    logging.info('Loading model files from folder: %s' % model_file)\n\n    checkpoint = torch.load(model_file, map_location=device)\n    model.load_state_dict(checkpoint)\n\n    del checkpoint  # dereference\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nglobal class_distr\n# Aggregate Distribution Mixed Water, Wakes, Cloud Shadows, Waves with Marine Water\nif options['agg_to_water']:\n    agg_distr = sum(class_distr[-4:])  # Density of Mixed Water, Wakes, Cloud Shadows, Waves\n    class_distr[6] += agg_distr       # To Water\n    class_distr = class_distr[:-4]    # Drop Mixed Water, Wakes, Cloud Shadows, Waves\n\n# Weighted Cross Entropy Loss & adam optimizer\nweight = gen_weights(class_distr, c=options['weight_param'])\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean', weight=weight.to(device))\n\noptimizer = torch.optim.Adam(model.parameters(), lr=options['lr'], weight_decay=options['decay'])\n\n# Learning Rate scheduler\nif options['reduce_lr_on_plateau'] == 1:\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\nelse:\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, options['lr_steps'], gamma=0.1, verbose=True)\n\n# Start training\nstart = options['resume_from_epoch'] + 1\nepochs = options['epochs']\neval_every = options['eval_every']\n\n# Write model-graph to Tensorboard\nt_loss = []\nv_loss = []\nv_acc = []\n\ndataiter = iter(train_loader)\nimage_temp, _ = next(dataiter)\n# writer.add_graph(model, image_temp.to(device))\n\n###############################################################\n# Start Training                                              #\n###############################################################\nmodel.train()\n\nfor epoch in range(start, epochs + 1):\n    training_loss = []\n    training_batches = 0\n\n    i_board = 0\n    for (image, target) in tqdm(train_loader, desc=\"training\"):\n        image = image.to(device)\n        target = target.to(device)\n\n        optimizer.zero_grad()\n\n        logits = model(image)\n\n        loss = criterion(logits, target)\n\n        loss.backward()\n\n        training_batches += target.shape[0]\n\n        training_loss.append((loss.data * target.shape[0]).tolist())\n\n        optimizer.step()\n\n        # Write running loss\n        writer.add_scalar('training loss', loss, (epoch - 1) * len(train_loader) + i_board)\n        i_board += 1\n\n    t_loss.append(sum(training_loss) / training_batches)\n    logging.info(\"Training loss was: \" + str(sum(training_loss) / training_batches))\n    print(f\"Epoch {epoch}: Training loss was: {sum(training_loss) / training_batches}\")\n\n    ###############################################################\n    # Start Evaluation                                            #\n    ###############################################################\n\n    if epoch % eval_every == 0 or epoch == 1:\n        model.eval()\n\n        test_loss = []\n        test_batches = 0\n        y_true = []\n        y_predicted = []\n\n        with torch.no_grad():\n            for (image, target) in tqdm(test_loader, desc=\"testing\"):\n                image = image.to(device)\n                target = target.to(device)\n\n                logits = model(image)\n\n                loss = criterion(logits, target)\n\n                # Accuracy metrics only on annotated pixels\n                logits = torch.movedim(logits, (0, 1, 2, 3), (0, 3, 1, 2))\n                logits = logits.reshape((-1, options['output_channels']))\n                target = target.reshape(-1)\n                mask = target != -1\n                logits = logits[mask]\n                target = target[mask]\n\n                probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n                target = target.cpu().numpy()\n\n                test_batches += target.shape[0]\n                test_loss.append((loss.data * target.shape[0]).tolist())\n                y_predicted += probs.argmax(1).tolist()\n                y_true += target.tolist()\n\n            y_predicted = np.asarray(y_predicted)\n            y_true = np.asarray(y_true)\n\n            ####################################################################\n            # Save Scores to the .log file and visualize also with tensorboard #\n            ####################################################################\n            acc = Evaluation(y_predicted, y_true)\n            logging.info(\"\\n\")\n            logging.info(\"Test loss was: \" + str(sum(test_loss) / test_batches))\n            logging.info(\"STATISTICS AFTER EPOCH \" + str(epoch) + \": \\n\")\n            logging.info(\"Evaluation: \" + str(acc))\n            print(f\"Epoch {epoch}: Test loss was: {sum(test_loss) / test_batches}\")\n            print(f\"STATISTICS AFTER EPOCH {epoch}: \\n\")\n            print(f\"Evaluation: {acc}\")\n            v_loss.append(sum(test_loss) / test_batches)\n            v_acc.append(acc['subsetAcc'])\n\n            logging.info(\"Saving models\")\n            model_dir = os.path.join(options['checkpoint_path'], str(epoch))\n            model_dir = options['checkpoint_path'] + str(epoch)\n            os.makedirs(model_dir, exist_ok=True)\n            torch.save(model.state_dict(), os.path.join(model_dir, 'model.pth'))\n\n            writer.add_scalars('Loss per epoch', {'Test loss': sum(test_loss) / test_batches,\n                                                  'Train loss': sum(training_loss) / training_batches},\n                               epoch)\n\n            writer.add_scalar('Precision/test macroPrec', acc[\"macroPrec\"], epoch)\n            writer.add_scalar('Precision/test microPrec', acc[\"microPrec\"], epoch)\n            writer.add_scalar('Precision/test weightPrec', acc[\"weightPrec\"], epoch)\n\n            writer.add_scalar('Recall/test macroRec', acc[\"macroRec\"], epoch)\n            writer.add_scalar('Recall/test microRec', acc[\"microRec\"], epoch)\n            writer.add_scalar('Recall/test weightRec', acc[\"weightRec\"], epoch)\n\n            writer.add_scalar('F1/test macroF1', acc[\"macroF1\"], epoch)\n            writer.add_scalar('F1/test microF1', acc[\"microF1\"], epoch)\n            writer.add_scalar('F1/test weightF1', acc[\"weightF1\"], epoch)\n\n            writer.add_scalar('IoU/test MacroIoU', acc[\"IoU\"], epoch)\n\n        if options['reduce_lr_on_plateau'] == 1:\n            scheduler.step(sum(test_loss) / test_batches)\n        else:\n            scheduler.step()\n\n        model.train()\n\n# plt.figure()\n# plt.plot(t_loss, 'r')\n# plt.plot(v_loss, 'b')\n# plt.legend(['train', 'validation'])\n# plt.figure()\n# plt.plot(v_acc, 'r')\n# Plotting the losses over epochs\nplt.figure()\nplt.plot(t_loss, 'r', label='Training Loss')\nplt.plot(v_loss, 'b', label='Validation Loss')\nplt.title('Training and Validation Loss Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('loss_plot.png')  # Optional: Save the plot\nplt.show()\n\n# Plotting the validation accuracy over epochs\nplt.figure()\nplt.plot(v_acc, 'r', label='Validation Accuracy')\nplt.title('Validation Accuracy Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.savefig('accuracy_plot.png')  # Optional: Save the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:32:43.679542Z","iopub.execute_input":"2024-12-02T14:32:43.680036Z","iopub.status.idle":"2024-12-02T14:33:44.746614Z","shell.execute_reply.started":"2024-12-02T14:32:43.679960Z","shell.execute_reply":"2024-12-02T14:33:44.744114Z"}},"outputs":[{"name":"stderr","text":"Load train set to memory: 100%|██████████| 694/694 [00:32<00:00, 21.07it/s]\nLoad val set to memory: 100%|██████████| 328/328 [00:15<00:00, 20.62it/s]\ntraining:   6%|▌         | 8/139 [00:10<02:54,  1.33s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[80], line 144\u001b[0m\n\u001b[1;32m    140\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    142\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 144\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, target)\n\u001b[1;32m    148\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[77], line 92\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m x7 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(x6, x3)\n\u001b[1;32m     91\u001b[0m x8 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3(x7, x2)\n\u001b[0;32m---> 92\u001b[0m x9 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Output Convolution Layer\u001b[39;00m\n\u001b[1;32m     95\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutc(x9)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[77], line 47\u001b[0m, in \u001b[0;36mUp.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     45\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup(x1)\n\u001b[1;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x2, x1], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:176\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2512\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2510\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2514\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":80},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"dataset_test = GenDEBRIS('test', transform=transform_test, standardization=standardization, agg_to_water=options['agg_to_water'])\n\ntest_loader = DataLoader(dataset_test,\n                        batch_size=1,\n                        shuffle=False,\n                        num_workers=options['num_workers'],\n                        pin_memory=options['pin_memory'],\n                        prefetch_factor=options['prefetch_factor'],\n                        persistent_workers=options['persistent_workers'],\n                        worker_init_fn=seed_worker,\n                        generator=g)\n\nmodel.eval()\n\ntest_loss = []\ntest_batches = 0\ny_true = []\ny_predicted = []\n\ni = 0\nwith torch.no_grad():\n    for (image, target) in tqdm(test_loader, desc=\"testing\"):\n        image = image.to(device)\n        target = target.to(device)\n\n        logits_ = model(image)\n        plt.figure()\n        plt.subplot(1, 3, 1), plt.imshow(np.squeeze(image.permute(0, 2, 3, 1).detach().cpu()[:, :, :, [3, 2, 1]]))\n        plt.subplot(1, 3, 2), plt.imshow(np.squeeze(torch.argmax(logits_, 1).detach().cpu()), cmap='viridis', clim=[0, 11])\n        plt.subplot(1, 3, 3), plt.imshow(np.squeeze(target.detach().cpu()), cmap='viridis', clim=[0, 11])\n\n        loss = criterion(logits_, target)\n\n        # Accuracy metrics only on annotated pixels\n        logits = torch.movedim(logits_, (0, 1, 2, 3), (0, 3, 1, 2))\n        logits = logits.reshape((-1, options['output_channels']))\n        target = target.reshape(-1)\n        mask = target != -1\n        logits = logits[mask]\n        target = target[mask]\n\n        probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n        target = target.cpu().numpy()\n\n        test_batches += target.shape[0]\n        test_loss.append((loss.data * target.shape[0]).tolist())\n        y_predicted += probs.argmax(1).tolist()\n        y_true += target.tolist()\n\n    y_predicted = np.asarray(y_predicted)\n    y_true = np.asarray(y_true)\n\n    ####################################################################\n    # Save Scores to the .log file                                     #\n    ####################################################################\n    acc = Evaluation(y_predicted, y_true)\n    logging.info(\"\\n\")\n    logging.info(\"Test loss was: \" + str(sum(test_loss) / test_batches))\n    logging.info(\"STATISTICS: \\n\")\n    logging.info(\"Evaluation: \" + str(acc))\n\n    # Print results to the console\n    print(f\"Test loss was: {sum(test_loss) / test_batches}\")\n    print(\"STATISTICS: \\n\")\n    print(f\"Evaluation: {acc}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:33:44.748671Z","iopub.status.idle":"2024-12-02T14:33:44.749421Z","shell.execute_reply.started":"2024-12-02T14:33:44.749060Z","shell.execute_reply":"2024-12-02T14:33:44.749094Z"}},"outputs":[],"execution_count":null}]}